[ Info: using system MPI
‚îå Info: Using implementation
‚îÇ   libmpi = "/opt/openmpi_gcc/4.0.4/lib/libmpi"
‚îÇ   mpiexec_cmd = `/opt/openmpi_gcc/4.0.4/bin/mpiexec`
‚îî   MPI_LIBRARY_VERSION_STRING = "Open MPI v4.0.4, package: Open MPI root@be02 Distribution, ident: 4.0.4, repo rev: v4.0.4, Jun 10, 2020\0"
‚îå Info: MPI implementation detected
‚îÇ   impl = OpenMPI::MPIImpl = 2
‚îÇ   version = v"4.0.4"
‚îî   abi = "OpenMPI"
‚îå Info:  Non-empty JULIA_PETSC_LIBRARY environment variable found.
‚îÇ Trying to use the PETSc installation it points to.
‚îî JULIA_PETSC_LIBRARY=/home/froca/software/petsc/3.15.5/lib/libpetsc.so
‚îå Info: PETSc configuration summary:
‚îÇ libpetsc_provider = JULIA_PETSC_LIBRARY
‚îÇ libpetsc_path     = /home/froca/software/petsc/3.15.5/lib/libpetsc.so
‚îÇ PetscReal         = Float64
‚îÇ PetscScalar       = Float64
‚îî PetscInt          = Int64
[ Info: Using the gmsh installation installed via BinaryBuilder.
[ Info: gmsh binary found in /home/froca/.julia/artifacts/4653460f3c6b714482628cbde55e1ca3845b68b4/bin/gmsh
[ Info: Using the gmsh Julia API found in /home/froca/.julia/artifacts/4653460f3c6b714482628cbde55e1ca3845b68b4/lib/gmsh.jl
    Building MPI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/d56a80d8cf8b9dc3050116346b3d83432b1912c0/build.log`
    Building GridapPETSc ‚Üí `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/df7b07e157c9a50f0c39a7ec00392584d6210384/build.log`
    Building GridapGmsh ‚îÄ‚Üí `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/d7e4e6b11b44a99aa400871e5f89be1be23814d2/build.log`
Precompiling project...
[32m  ‚úì [39mMPI
[32m  ‚úì [39mPartitionedArrays
[32m  ‚úì [39mGridapDistributed
[32m  ‚úì [39mGridapPETSc
[32m  ‚úì [39mGridapGmsh
[32m  ‚úì [39mGridapMHD
  6 dependencies successfully precompiled in 38 seconds (79 already precompiled)
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              be02
  Local adapter:           mlx4_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   be02
  Local device: mlx4_0
--------------------------------------------------------------------------
[be02:32165] 8 more processes have sent help message help-mpi-btl-openib.txt / ib port not selected
[be02:32165] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[be02:32165] 8 more processes have sent help message help-mpi-btl-openib.txt / error in device init
‚îå Error: 
‚îÇ   exception =
‚îÇ    UndefVarError: kmap not defined
‚îÇ    Stacktrace:
‚îÇ     [1] _hunt(; parts::PartitionedArrays.MPIData{Int64, 3}, nc::Tuple{Int64, Int64}, ŒΩ::Float64, œÅ::Float64, œÉ::Float64, B::Tuple{Float64, Float64, Float64}, f::Tuple{Float64, Float64, Float64}, L::Float64, u0::Float64, B0::Float64, nsums::Int64, vtk::Bool, title::String, path::String, debug::Bool, solver::Symbol, verbose::Bool, kmap_s::Int64, kmap_Ha::Int64, petsc_options::String)
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:216
‚îÇ     [2] #12
‚îÇ       @ /projects/blankets/GridapMHD.jl/src/Hunt.jl:19 [inlined]
‚îÇ     [3] prun(driver::GridapMHD.var"#12#14"{String, Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}}, String}, b::PartitionedArrays.MPIBackend, nparts::Tuple{Int64, Int64, Int64})
‚îÇ       @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:33
‚îÇ     [4] hunt(; backend::Symbol, np::Tuple{Int64, Int64}, parts::Nothing, title::String, nruns::Int64, path::String, kwargs::Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}})
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:18
‚îÇ     [5] top-level scope
‚îÇ       @ none:3
‚îÇ     [6] eval
‚îÇ       @ ./boot.jl:373 [inlined]
‚îÇ     [7] exec_options(opts::Base.JLOptions)
‚îÇ       @ Base ./client.jl:268
‚îÇ     [8] _start()
‚îÇ       @ Base ./client.jl:495
‚îî @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:35
‚îå Error: 
‚îÇ   exception =
‚îÇ    UndefVarError: kmap not defined
‚îÇ    Stacktrace:
‚îÇ     [1] _hunt(; parts::PartitionedArrays.MPIData{Int64, 3}, nc::Tuple{Int64, Int64}, ŒΩ::Float64, œÅ::Float64, œÉ::Float64, B::Tuple{Float64, Float64, Float64}, f::Tuple{Float64, Float64, Float64}, L::Float64, u0::Float64, B0::Float64, nsums::Int64, vtk::Bool, title::String, path::String, debug::Bool, solver::Symbol, verbose::Bool, kmap_s::Int64, kmap_Ha::Int64, petsc_options::String)
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:216
‚îÇ     [2] #12
‚îÇ       @ /projects/blankets/GridapMHD.jl/src/Hunt.jl:19 [inlined]
‚îÇ     [3] prun(driver::GridapMHD.var"#12#14"{String, Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}}, String}, b::PartitionedArrays.MPIBackend, nparts::Tuple{Int64, Int64, Int64})
‚îÇ       @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:33
‚îÇ     [4] hunt(; backend::Symbol, np::Tuple{Int64, Int64}, parts::Nothing, title::String, nruns::Int64, path::String, kwargs::Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}})
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:18
‚îÇ     [5] top-level scope
‚îÇ       @ none:3
‚îÇ     [6] eval
‚îÇ       @ ./boot.jl:373 [inlined]
‚îÇ     [7] exec_options(opts::Base.JLOptions)
‚îÇ       @ Base ./client.jl:268
‚îÇ     [8] _start()
‚îÇ       @ Base ./client.jl:495
‚îî @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:35
‚îå Error: 
‚îÇ   exception =
‚îÇ    UndefVarError: kmap not defined
‚îÇ    Stacktrace:
‚îÇ     [1] _hunt(; parts::PartitionedArrays.MPIData{Int64, 3}, nc::Tuple{Int64, Int64}, ŒΩ::Float64, œÅ::Float64, œÉ::Float64, B::Tuple{Float64, Float64, Float64}, f::Tuple{Float64, Float64, Float64}, L::Float64, u0::Float64, B0::Float64, nsums::Int64, vtk::Bool, title::String, path::String, debug::Bool, solver::Symbol, verbose::Bool, kmap_s::Int64, kmap_Ha::Int64, petsc_options::String)
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:216
‚îÇ     [2] #12
‚îÇ       @ /projects/blankets/GridapMHD.jl/src/Hunt.jl:19 [inlined]
‚îÇ     [3] prun(driver::GridapMHD.var"#12#14"{String, Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}}, String}, b::PartitionedArrays.MPIBackend, nparts::Tuple{Int64, Int64, Int64})
‚îÇ       @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:33
‚îÇ     [4] hunt(; backend::Symbol, np::Tuple{Int64, Int64}, parts::Nothing, title::String, nruns::Int64, path::String, kwargs::Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}})
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:18
‚îÇ     [5] top-level scope
‚îÇ       @ none:3
‚îÇ     [6] eval
‚îÇ       @ ./boot.jl:373 [inlined]
‚îÇ     [7] exec_options(opts::Base.JLOptions)
‚îÇ       @ Base ./client.jl:268
‚îÇ     [8] _start()
‚îÇ       @ Base ./client.jl:495
‚îî @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:35
‚îå Error: 
‚îÇ   exception =
‚îÇ    UndefVarError: kmap not defined
‚îÇ    Stacktrace:
‚îÇ     [1] _hunt(; parts::PartitionedArrays.MPIData{Int64, 3}, nc::Tuple{Int64, Int64}, ŒΩ::Float64, œÅ::Float64, œÉ::Float64, B::Tuple{Float64, Float64, Float64}, f::Tuple{Float64, Float64, Float64}, L::Float64, u0::Float64, B0::Float64, nsums::Int64, vtk::Bool, title::String, path::String, debug::Bool, solver::Symbol, verbose::Bool, kmap_s::Int64, kmap_Ha::Int64, petsc_options::String)
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:216
‚îÇ     [2] #12
‚îÇ       @ /projects/blankets/GridapMHD.jl/src/Hunt.jl:19 [inlined]
‚îÇ     [3] prun(driver::GridapMHD.var"#12#14"{String, Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}}, String}, b::PartitionedArrays.MPIBackend, nparts::Tuple{Int64, Int64, Int64})
‚îÇ       @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:33
‚îÇ     [4] hunt(; backend::Symbol, np::Tuple{Int64, Int64}, parts::Nothing, title::String, nruns::Int64, path::String, kwargs::Base.Pairs{Symbol, Any, NTuple{8, Symbol}, NamedTuple{(:nc, :L, :B, :debug, :vtk, :solver, :kmap_s, :kmap_Ha), Tuple{Tuple{Int64, Int64}, Float64, Tuple{Float64, Float64, Float64}, Bool, Bool, Symbol, Int64, Int64}}})
‚îÇ       @ GridapMHD /projects/blankets/GridapMHD.jl/src/Hunt.jl:18
‚îÇ     [5] top-level scope
‚îÇ       @ none:3
‚îÇ     [6] eval
‚îÇ       @ ./boot.jl:373 [inlined]
‚îÇ     [7] exec_options(opts::Base.JLOptions)
‚îÇ       @ Base ./client.jl:268
‚îÇ     [8] _start()
‚îÇ       @ Base ./client.jl:495
‚îî @ PartitionedArrays ~/.julia/packages/PartitionedArrays/fem1y/src/MPIBackend.jl:35
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[be02:32165] 3 more processes have sent help message help-mpi-api.txt / mpi-abort
